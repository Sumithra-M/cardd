# -*- coding: utf-8 -*-
"""Car_Damage_Image_Multi_Class_Classification review 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JrUx0ZOGJ7bINVSa9nZmey8HdxcK2esG
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'analytics-vidya-ripik-ai-hackfest:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4118403%2F7136937%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240403%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240403T095052Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D692f73d8c94be7527024893642f734a48b62db6727aced539a4988dec140644a90035a1cd9960802fe1473e29e36c4c8a462fb9163ea30581729af418e7717a9778cfb7553b52bc9e6c747e64d99b75c98812e9d1caaa2b4f93bca997fb4e26948496b2fa307f74d987164176f20169ae446eb966a07003fbc63b300c70b5b3102f1996838ec5f32c9520565ac5673551f8b9a2f6c9e404eb73c77c0b29f8f231d7913e06656757ca20e541fca2879ff051bf0f370b01205e0a4a4944940e86a470f2fde2cae189dca0fa3b5675e419af1ef93a873caa514bd1e15bcc8c57f38852633ae702a708f53c0f640e56a5714fb29dda84e48376ce69759d2f8799cb6'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""<div style="align: center;">
    <br>
    <img src="https://i.imgur.com/1SpvMr3.jpg" style="display:block; margin:auto; width:55%; height:210px;">
</div><br><br>

<div style="letter-spacing:normal; opacity:1.;">
<!--   https://xkcd.com/color/rgb/   -->
  <p style="text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace;
            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;">
            <strong>Car Damage Image Classification</strong></p>  
  
  <p style="text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace;
            line-height:1.0; font-size:28px; font-weight:normal; text-transform: capitalize; padding: 5px;"
     >Deep Learning Module<br>Car Damage Image Multi-Class Classification
      <br>EDA and <a href='https://keras.io/api/applications/'>"EfficientNetV2"</a> Model Training<br>(Convolutional Neural Network (CNN))</p>    
</div>

- https://keras.io/api/applications/
- https://github.com/leondgarse/keras_cv_attention_models

## Introduction

In the insurance industry, processing claims for vehicle damage is a common task.<br>
With advancements in AI and Computer Vision, settling claims online by uploading damaged car images is now possible.

## Dataset

> https://www.kaggle.com/datasets/imnandini/analytics-vidya-ripik-ai-hackfest
>> Training set (`train.zip`)<br>
>> Test set (`test.zip`)<br>
>> Sample submission (`sample_submission.csv`)

### Training Dataset

The training set contains a diverse dataset of car images with labels indicating the specific type of damage (e.g., dents, scratches, cracks).<br>
The `train.csv` file includes the following columns:

- `image_id`: Unique identifier of the image<br>
- `filename`: Filename of the image<br>
- `label`: Type of damage present in the car<br>
  1. Crack
  2. Scratch
  3. Tire Flat
  4. Dent
  5. Glass Shatter
  6. Lamp Broken

### Test Dataset

The test set contains only images, and the goal is to predict the type of damage for each image.<br>
The `test.csv` file includes the following columns:

- `image_id`: Unique identifier of the image
- `filename`: Filename of the image

## Sample Submission

The solution file must contain predictions for every `image_id` in the test set. It must contain only 2 columns - `image_id` and `label`.<br>
The solution file format must be similar to that of `sample_submission.csv`. `sample_submission.csv` contains 2 variables:

- `image_id`: Unique identifier of an image
- `label`: Type of damage present in the car {1:crack, 2:scratch, 3:tire flat, 4:dent, 5:glass shatter, 6:lamp broken}

## Evaluation Metric

The model will be evaluated based on the macro F1 score.

## Table of Contents

1. Recognizing and Understanding Data
    - Reading train and test csv Files
    - Detailed EDA Check Images
    
    
2. Train Test Split


3. Build Dataset
    - Image Data Generator (Data augmentation)
    - flow_from_dataframe (with pandas dataframe)
    - flow_from_directory (with subdirectories)
    - Dataset Sanity Check
    - Dataset Benchmark Test
    - Optimize Performance
    - Explore Dataset Images
    
    
3. CNN (CONVOLUTIONAL NEURAL NETWORK) MODELING
    - Build CNN Model
    - Model Training
    - Model History
        - Save Model Weights
    - Model evaluation
        - BEST THRESHOLD calculate precision-recall curve
        - Predict Val Data
        - Predict Test Data
        
        
4. Save Submission

<div style="letter-spacing:normal; opacity:1.;">
  <h1 style="text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;
            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;">
            <strong>1. Import Libraries & Ingest Data</strong></h1>   
</div>

<h4>pip freeze</h4>

## Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# # Install packages, Most of exist on Kaggle env
# # tensorflow[and-cuda]
# tflite-runtime
# keras-image-helper
# 
# matplotlib
# ipywidgets
# tqdm
# pipenv
# session_info

import os, sys, platform

!{sys.executable} -m pip install -r requirements.txt --no-cache-dir -Uq
print("Platform:", platform.system())  # platform.platform()
print("Python  :", platform.python_version())  # sys.version
print("Actv Env:", os.getenv('CONDA_DEFAULT_ENV', 'Not Found Conda Env'))

"""## Pipenv for dependency management:"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# get_versions() {
#   for lib in "$@"; do
#     echo "$lib==$(pip show $lib | grep Version | cut -d ' ' -f 2)"
#   done
# }
# # Example usage
# get_versions  "tflite-runtime" "keras-image-helper" "requests" "Pillow" "numpy" "pandas" "matplotlib" > requirements.txt
# cat requirements.txt

# Generate Pipfile file from requirements.txt
!pipenv --python 3.10
# Create a virtual environment and install packages
!pipenv install
# Update the Pipfile.lock
!pipenv lock
# Clean up the temporary virtual environment
!pipenv --rm
# Check
# !ls ~/.local/share/virtualenvs/
!cat Pipfile

"""## Print Available Devices - 'TPU', 'GPU', 'CPU'."""

import os
# Disable TensorFlow warnings, before you import tensorflow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow as tf
print("Tensorflow version \t\t:", tf.__version__)

# Determines the number of threads used by independent non-blocking operations.
# Tensorflow set number of threads to 1 for speed up in parallell function mapping.
# 0 means the system picks an appropriate number.
tf.config.threading.set_inter_op_parallelism_threads(num_threads=1)
print("Number of threads \t\t:", tf.config.threading.get_inter_op_parallelism_threads())

print("Available devices:")
# tf.config.list_physical_devices('GPU')
for i, device in enumerate(tf.config.list_logical_devices()):
    print("%d) %s" % (i, device))

try:
    # If there's a GPU avaiable, to use the GPU, otherwise, using the CPU instead.
    gpus = tf.config.list_logical_devices('GPU')
    if len(gpus) > 1:
        strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])
        print('Running on multiple GPUs ', gpus)
    elif len(gpus) == 1:
        # default strategy that works on CPU and single GPU
        strategy = tf.distribute.get_strategy()
        print('Running on single GPU ', gpus[0].name)
    else:
        # default strategy that works on CPU and single GPU
        strategy = tf.distribute.get_strategy()
        print('Running on CPU')
finally:
    print("Number of accelerators: ", strategy.num_replicas_in_sync)

# Enable Just-In-Time (JIT) computation graph at runtime, just before they are executed.
# The idea is to convert the computation graph into machine code optimized for the specific hardware it will run on.
tf.config.optimizer.set_jit(True)
tf.config.optimizer.get_jit()

!pip install scikit-plot

"""# Importing Related Libraries"""

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
# !pip install scikit-plot -Uq
import scikitplot as skplt

import tflite_runtime.interpreter as tflite
from keras_image_helper import create_preprocessor
from PIL import Image
import requests
import io

import re
import time
import shutil
import random
import datetime
import tempfile
import importlib
from glob import glob
from typing import cast
from pathlib import Path
from tqdm import tqdm
from multiprocessing import cpu_count
import pickle

import gc
gc.collect()

# !pip install session_info -Uq
import session_info
session_info.show(html=False)

"""**WARNING**: To prevent fragmentation and other memory errors, batch size has been adjusted based on Kaggle GPU memory 15 GB.

I would not recommend attempting to run this project without a GPU enabled tensorflow build as it will take hours or days to run.

However, TPU can also be used (TPUs can be ~100x faster than CPUs and ~3.5x faster than GPUs).
"""

import os

DATA_DIR = '/kaggle/input/analytics-vidya-ripik-ai-hackfest'
print('DATA_DIR :', DATA_DIR)

# Set epochs
N_EPOCHS = 3
# Set the initial learning rate and weight decay, lr=0.001, wd=0.004
LEARNING_RATE = 1e-3
print('LEARNING_RATE:', '{:.1e}'.format(LEARNING_RATE))

# Reduce BATCH_SIZE or model complexity: If you are encountering ERRORS
BATCH_SIZE = 8
# You can adjust the batch size based on your requirements.
print('BATCH_SIZE   :', BATCH_SIZE)

# Is Interactive Flag and Corresponding Verbosity Method
IS_INTERACTIVE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') == 'Interactive'
VERBOSE = 1 if IS_INTERACTIVE else 0

"""<div style="letter-spacing:normal; opacity:1.;">
  <h1 style="text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;
            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;">
            <strong>2. Recognizing and Understanding Data</strong></h1>   
</div>

## Read csv files
"""

df_train = pd.read_csv(f'{DATA_DIR}/train/train/train.csv')
df_test  = pd.read_csv(f'{DATA_DIR}/test/test/test.csv')

df_train.shape, df_test.shape

display(df_train, df_test)

display(df_train.info(), df_test.info())

"""## Check class distrubituon, If not enough, eliminate"""

label_to_cls={1:"crack",
              2:"scratch",
              3:"tire flat",
              4:"dent",
              5:"glass shatter",
              6:"lamp broken"}

cls_to_label={"crack":1,
              "scratch":2,
              "tire flat":3,
              "dent":4,
              "glass shatter":5,
              "lamp broken":6}

df_train.label.unique()

# Train data is imbalanced
df_train['label'].map({k:f'{k}\n{v}' for k,v in label_to_cls.items()}).value_counts().plot(
    kind='bar', title='Label Distribution', ylabel='count', legend=True,
    fontsize=14, rot=0,
    figsize=(12, 4),
)
plt.bar_label(plt.gca().containers[0])
plt.show()

"""**Note**: For the multi-class classification task we can choose **"scratch", "dent", "glass shatter"**.

## Random sampling for quality checks
"""

# Function to display random images from an each class
def display_random_images(num_images=1):
    df = df_train.groupby('label').sample(num_images).sort_values(by='label')
    num_class = df.label.unique().size

    # Display the sampled images
    fig, ax = plt.subplots(nrows=num_images, ncols=num_class, figsize=(15,15))
    for g, group in df.groupby('label'):
        for idx, (_, row) in enumerate(group.iterrows()):
            filename = row["filename"]
            label = row["label"]
            label_class = label_to_cls[label]

            image_dir  = f"{DATA_DIR}/train/train/images"
            image_path = os.path.sep.join([image_dir, filename])
            image_data = tf.keras.preprocessing.image.load_img(image_path)

            ax[idx, label-1].imshow(image_data)  # Switched indices
            ax[idx, label-1].set_title(f'class: {label_class.upper()}\nimage: {filename}')
            ax[idx, label-1].axis('off')
    plt.suptitle('Random Images Displayed by Each Class', fontsize=16)
    plt.tight_layout()
    plt.show()

# Call the function
display_random_images(5)

"""## Image size distribution"""

for g, group in df_train.groupby('label'):
    sizes = []
    for idx, (_, row) in enumerate(group.iterrows()):
        filename = row["filename"]
        label = row["label"]
        label_class = label_to_cls[label]

        image_dir = f"{DATA_DIR}/train/train/images"
        image_path = os.path.sep.join([image_dir, filename])
        image_data = tf.keras.preprocessing.image.load_img(image_path)
        sizes.append(image_data.size)

    df = pd.DataFrame(sizes, columns=['width', 'height'])
    df['quantile'] = pd.qcut(df.mean(axis=1), q=5, labels=False)
    df['count']    = df.groupby('quantile')['quantile'].transform('count')
    average_by_quantile = df.groupby('quantile').mean().astype(int)

    # Print size and count information for each class
    print(f'Class: {label_class}')
    print(f'{"Avg Size":<15}\t{"Quantile Count":<15}')
    print('-' * 30)
    for quantile, row in average_by_quantile.iterrows():
        print(f"{row['width']}x{row['height']:<15}\t{row['count']:<15}", end='\n\n')

"""Note:

- Average Size can be select: **1024x768**

- Minimum Size can be select: **800x600**

For fast processing can be use **512x512**. But this will negatively affect the scores.
"""

# Config
TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS = ( 800, 600, 3 )
INPUT_SHAPE  = (TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS)

sample = tf.keras.preprocessing.image.load_img(
    "/kaggle/input/analytics-vidya-ripik-ai-hackfest/train/train/images/2.jpg",
    target_size=INPUT_SHAPE
)
print("min-max:", np.array(sample).min(), np.array(sample).max())
print("shape  :", np.array(sample).shape)
sample

img_array = np.random.rand(BATCH_SIZE, *INPUT_SHAPE).astype(np.float32)
memory_size_mb = img_array.nbytes / (1024**2)
print(f"Memory size of the batch: {memory_size_mb:.2f} MB")

"""# Train Test Split"""

class_counts = df_train['label'].value_counts().reset_index()
# Filter classes with counts below 1000
filtered_classes = class_counts[class_counts['label'] > 1000]['index'].tolist()
# Filter the original DataFrame based on the selected classes
filtered_df = df_train[df_train['label'].isin(filtered_classes)]
filtered_df.label.unique()

# prepare DataFrame
df = df_train.copy()
df = df.assign(label=df.label.map(label_to_cls))
df.label.unique().tolist()

from sklearn.model_selection import train_test_split
SEED = 42

# Split the data into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.25, stratify=df['label'], random_state=SEED)
train_df.shape, val_df.shape

"""# Build Dataset

## Image Data Generator with Data augmentation

- Generate batches of tensor image data with real-time data augmentation.
- Image transformations:
    * Flip - Horizontal and Vertical
    * Rotation (note you may lose some of image)
    * Shifting - Heigh and Width (note you may lose some of image)
    * Shear ie pulling from corners
    * Zoom - X and Y axis or on both
"""

# Define the ImageDataGenerator with augmentations
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,    # Flip images horizontally
    vertical_flip=True,      # Flip images vertically
    rotation_range=5,        # Rotate images randomly up to 5 degrees
    zoom_range=0.3,          # Zoom in randomly up to 20%
    shear_range=0.05,        # Apply shear transformation
    width_shift_range=0.05,  # Shift images horizontally by up to 10% of the width
    height_shift_range=0.05, # Shift images vertically by up to 10% of the height
    fill_mode='nearest',     # Fill in missing pixels using the nearest available pixel
    preprocessing_function=None,
    # validation_split=0.2,
)
val_datagen  = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

"""## flow_from_dataframe

- Takes the dataframe and the path to a directory + generates batches.

- The generated batches contain augmented/normalized data.
"""

# Image with Augmentation
train_generator = train_datagen.flow_from_dataframe(
    train_df,
    directory='/kaggle/input/analytics-vidya-ripik-ai-hackfest/train/train/images',
    x_col='filename',
    y_col='label',
    class_mode='categorical',  # Set to 'categorical' for multi-class classification
    target_size=INPUT_SHAPE[:-1],
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    shuffle=True,  # Set to True if you want to shuffle the order of the images
)
# Image
val_generator = val_datagen.flow_from_dataframe(
    val_df,
    directory='/kaggle/input/analytics-vidya-ripik-ai-hackfest/train/train/images',
    x_col='filename',
    y_col='label',
    class_mode='categorical',  # Set to 'categorical' for multi-class classification
    target_size=INPUT_SHAPE[:-1],
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    shuffle=False,  # Set to True if you want to shuffle the order of the images
)
# Image
test_generator = test_datagen.flow_from_dataframe(
    df_test,
    directory='/kaggle/input/analytics-vidya-ripik-ai-hackfest/test/test/images',
    x_col='filename',
    y_col=None,
    class_mode=None,  # Set to 'categorical' for multi-class classification
    target_size=INPUT_SHAPE[:-1],
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    shuffle=False,  # Set to True if you want to shuffle the order of the images
)

"""## flow_from_directory

- Takes the path to a directory & generates batches of augmented data.
"""

# clear /kaggle/working
!rm -rf "train" "val"

# Build train Folder for ImageDataGenerator.flow_from_directory
for idx, df in train_df.iterrows():
    img_dir     = f"{DATA_DIR}/train/train/images"
    filename    = df["filename"]
    label_class = df["label"]

    img_path = os.path.sep.join([img_dir, filename])
    dest_dir = os.path.sep.join(["train", label_class])
    dest_path= os.path.sep.join([dest_dir, filename])
    os.makedirs(dest_dir, exist_ok=True)
    shutil.copy2(img_path, dest_path)


# Build val Folder for ImageDataGenerator.flow_from_directory
for idx, df in val_df.iterrows():
    img_dir     = f"{DATA_DIR}/train/train/images"
    filename    = df["filename"]
    label_class = df["label"]

    img_path = os.path.sep.join([img_dir, filename])
    dest_dir = os.path.sep.join(["val", label_class])
    dest_path= os.path.sep.join([dest_dir, filename])
    os.makedirs(dest_dir, exist_ok=True)
    shutil.copy2(img_path, dest_path)

train_generator2 = train_datagen.flow_from_directory(
    '/kaggle/input/analytics-vidya-ripik-ai-hackfest/train',
    class_mode='categorical',  # Set to 'categorical' for multi-class classification
    target_size=INPUT_SHAPE[:-1],
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    shuffle=True,  # Set to True if you want to shuffle the order of the images
    seed=SEED,
    # subset="training",
)
val_generator2 = val_datagen.flow_from_directory(
    '/kaggle/input/analytics-vidya-ripik-ai-hackfest/train',
    class_mode='categorical',  # Set to 'categorical' for multi-class classification
    target_size=INPUT_SHAPE[:-1],
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    shuffle=True,  # Set to True if you want to shuffle the order of the images
    seed=SEED,
)
test_generator2 = test_datagen.flow_from_directory(
    '/kaggle/input/analytics-vidya-ripik-ai-hackfest/test/test',
    class_mode=None,  # No class mode for the test set
    target_size=INPUT_SHAPE[:-1],
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    shuffle=False,  # Do not shuffle the test set
    seed=SEED,
)

train_generator.class_indices, train_generator2.class_indices

"""## Dataset Sanity Check"""

# Sanity check, image and label statistics
X_batch, y_batch = next(iter(train_generator))

print(f'image shape  : {X_batch.shape}, image dtype: {X_batch.dtype}')
print(f'image min    : {X_batch.min():.2f}, max: {X_batch.max():.2f}')
print(f'y_batch shape: {y_batch.shape}, y_batch dtype: {y_batch.dtype}')

"""## Dataset Benchmark Test"""

import time
def benchmark_dataset(data_generator, num_epochs=2, steps_per_epoch=5, batch_size=BATCH_SIZE):
    # Iterate over epochs
    for epoch_num in range(num_epochs):
        epoch_start_time = time.perf_counter()
        # Iterate over steps in the epoch
        for step_idx in range(steps_per_epoch + 1):
            try:
                inputs, labels = next(data_generator)
            except StopIteration:
                break
            # Print information for the first step of the first epoch
            if epoch_num == 0 and step_idx == 0:
                print(f'Image shape : {str(inputs.shape):<{len(str(inputs.shape))}}, dtype: {inputs.dtype}\n'
                      f'Labels shape: {str(labels.shape):<{len(str(inputs.shape))}}, dtype: {labels.dtype}\n')

        # Calculate and print epoch metrics
        epoch_duration     = time.perf_counter() - epoch_start_time
        mean_step_duration = round(epoch_duration / steps_per_epoch * 1000, 1)
        images_per_second  = int(1 / (mean_step_duration / 1000) * batch_size)
        print(f'Epoch {epoch_num} took: {round(epoch_duration, 2)} sec, '
              f'Mean step duration: {mean_step_duration}ms, '
              f'Images/s: {images_per_second}')

# Benchmark Dataset
benchmark_dataset(train_generator)

# Benchmark Dataset
benchmark_dataset(train_generator2)

"""## Optimize Performance

- Pass
- it usuful for for tf.data.Dataset cache, prefect etc.

## Explore Dataset Images
"""

def show_batch(X_batch, y_batch=None, n_images=[5, 5], batch_size=BATCH_SIZE):
    if y_batch is not None:
        print(f"y ratio:")
        # Use np.unique to get unique values and their counts
        values, counts = np.unique(np.argmax(y_batch, axis=1), return_counts=True)
        for value, count in zip(values.astype(np.uint8), counts):
            print(f"\tClass: {value}, Count: {count}")

    fig, axes = plt.subplots(
        nrows=n_images[0], ncols=n_images[1],
        figsize=(n_images[1]*5, n_images[0]*5))
    fig.subplots_adjust(hspace=0.1, wspace=0.01)

    for n, ax in enumerate(axes.flat):
        # Explicitly remove overlapping axes
        if n >= batch_size:
            for i in range(batch_size, np.prod(n_images)):
                axes.flat[i].remove()
            break
        else:
            ax.imshow(X_batch[n]) # binary, gray, bone
            ax.axis('off'); ax.set_title(f"test")
            if y_batch is not None:
                ax.set_title(f"label: {y_batch[n]}\n"
                             f"size: {X_batch[n].shape}")
    fig.tight_layout()
    fig.show();


# Show Example Batch
X_batch, y_batch = next(iter(train_generator))
show_batch(X_batch, y_batch)
plt.suptitle("AUGMENTED: Training Dataset from Directory", y=1.01, fontsize=28, family='monospace');

"""# Model Building

## Evaluation Criteria

Submissions are evaluated using the [probabilistic F1 score](https://aclanthology.org/2020.eval4nlp-1.9.pdf) (pF1).<br>
This extension of the traditional F score accepts probabilities instead of binary classifications.<br>
You can find a Python implementation [here](https://www.kaggle.com/code/sohier/probabilistic-f-score).


> https://en.wikipedia.org/wiki/F-score
>> F1 = 2 * (precision * recall) / (precision + recall)<br>
>> F_beta = (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)<br>
>> F_beta = (1 + beta^2) * true_positive / ((1 + beta^2) * true_positive + beta^2 * false_negative + false_positive)<br>

```py
# https://www.kaggle.com/code/sohier/probabilistic-f-score
def pfbeta(labels, predictions, beta):
    y_true_count = 0
    ctp = 0  # Correctly predicted positives
    cfp = 0  # Incorrectly predicted positives

    for idx in range(len(labels)):
        # Ensure the prediction is between 0 and 1
        prediction = min(max(predictions[idx], 0), 1)

        # Count the true positives and false positives
        if labels[idx]:
            y_true_count += 1
            ctp += prediction
        else:
            cfp += prediction

    beta_squared = beta * beta
    c_precision = ctp / (ctp + cfp)  # Precision for the current threshold
    c_recall = ctp / y_true_count  # Recall for the current threshold

    # Calculate the F-beta score using the provided formula
    if c_precision > 0 and c_recall > 0:
        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)
        return result
    else:
        return 0
```
"""

# Tensorflow custom metric is just a conventional class object
class pFbeta(tf.keras.metrics.Metric):
    def __init__(self, beta=1, name='pFbeta', **kwargs):
        # Initialize properties
        super(pFbeta, self).__init__(name=name, **kwargs)
        self.beta = beta
        self.tp = self.add_weight(name='true_positives', initializer=tf.zeros_initializer)
        self.fp = self.add_weight(name='false_positives', initializer=tf.zeros_initializer)
        # self.tn = self.add_weight(name='true_negatives', initializer=tf.zeros_initializer)
        # self.fn = self.add_weight(name='false_negatives', initializer=tf.zeros_initializer)
        # self.tp + self.fn = self.tc
        self.tc = self.add_weight(name='actual_positives', initializer=tf.zeros_initializer)

    def update_state(self, y_true, y_pred, sample_weight=None):
        # Update state called on each batch with true and predicted labels
        self.tp.assign_add(tf.cast(tf.reduce_sum(y_pred[y_true == 1]), tf.float32))
        self.fp.assign_add(tf.cast(tf.reduce_sum(y_pred[y_true == 0]), tf.float32))
        # self.tn.assign_add(tf.cast(tf.reduce_sum(1 - y_pred[y_true == 0]), tf.float32))
        # self.fn.assign_add(tf.cast(tf.reduce_sum(1 - y_pred[y_true == 1]), tf.float32))
        self.tc.assign_add(tf.cast(tf.reduce_sum(y_true), tf.float32))

    def result(self):
        # Result function is called to obtain the result
        if self.tc == 0 or (self.tp + self.fp) == 0:
            # (self.tp + self.fn == 0) or (self.tp + self.fp == 0)
            return 0.0
        else:
            precision = self.tp / (self.tp + self.fp)
            recall = self.tp / self.tc
            beta_squared = self.beta ** 2
            fbeta_numerator = (1 + beta_squared) * (precision * recall)
            fbeta_denominator = beta_squared * precision + recall
            return fbeta_numerator / fbeta_denominator

    def reset_state(self):
        # Reset state is called after each epoch to start fresh each epoch
        self.tp.assign(0)
        self.fp.assign(0)
        self.tc.assign(0)

"""## Base Model with Transfer Learning: Model 1

- Keras offers pretrained models at [keras.io](https://keras.io/api/applications/)

- I use the [EfficientNetV2B0](https://keras.io/api/applications/convnext) model due to its fairly high Top-1 Accuracy and does not require depth.

- EfficientNetV2 models expect their inputs to be float tensors of pixels with values in the [0, 255] range.
"""

def model_builder(input_shape=INPUT_SHAPE, n_class=6):
    # Adding the InputLayer with the specified input_shape
    input_image = tf.keras.layers.Input(shape=input_shape, dtype=tf.float32, name='image')

    # Freeze the weights of Model, we train the model we don't want to change top layers are well trained.
    # base_model.trainable = False
    # Load Keras Model as basemodel (convolutional layers) ie the "bottom" in keras terms
    base_model_output = tf.keras.applications.EfficientNetV2B0(
        input_shape=input_shape,   # Specifying the expected input shape
        weights='imagenet',        # (pre-training on ImageNet-1k), or the path to the weights file to be loaded. Defaults to "imagenet"
        include_top=False,         # include the fully-connected layer at the top of the network
        # pooling='avg',           # means that global average pooling will be applied to the output of the last convolutional layer
    )(input_image, training=False) # Freeze the weights during this call, don't want to change top layers

    # Flatten layer, output of the model will be a 2D tensor. ( BxHxWxC to BxC )
    x = tf.keras.layers.GlobalAveragePooling2D()(base_model_output)

    # Start of Fully Connected (Dense) Layers: These layers connect every neuron to every neuron in the previous and subsequent layers.
    x = tf.keras.layers.Dense(256, activation='selu')(x)
    x = tf.keras.layers.Dropout(rate=0.3)(x)

    # End with output layer, n_class due to the number of classes
    output = tf.keras.layers.Dense(n_class, activation='softmax')(x)

    # Create the final model
    model = tf.keras.Model(inputs=input_image, outputs=output)
    # Compile the model with Adam optimizer and categorical crossentropy loss
    model.compile(
        # other optimizers https://keras.io/api/optimizers
        # Adam is currently the recommended starting point
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        # in the dense layers the model takes inputs, generates outputs and then there is a softmax aka activation applied
        # from_logits = False means we APPLY activation in output and get normalized Probability distribution [0..1]
        # from_logits = True means we DONT APPLY activation in output and get raw score aka aren't normalized Logits [-inf...+inf]
        # tf.keras.losses.CategoricalCrossentropy(from_logits=False)
        loss='categorical_crossentropy',
        metrics=['categorical_accuracy', pFbeta(), 'Precision', 'Recall']
    )
    return model

tf.keras.backend.clear_session()
gc.collect()

with strategy.scope():
    model = model_builder()
    model.summary()

# Training the model
history = model.fit(
    train_generator, validation_data=val_generator,
    epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=1,
)

plt.plot(history.history['categorical_accuracy'], label='train')
plt.plot(history.history['val_categorical_accuracy'], label='val')
plt.xticks(np.arange(3))
plt.legend();

"""## Tune Model: Model 2

- Change Learning Rate
- Adding more layers
    - Conv2D
    - AveragePooling2D
    - SpatialDropout2D
    - Dropout
    - BatchNormalization

Can be check KerasTuner API for more:
- BayesianOptimization Tuner

```py
import keras_tuner as kt
```
"""

# class_weights = (train_df.label.value_counts().max() / train_df.label.value_counts()).rename(train_generator.class_indices).to_dict()
# class_weights

def model_builder(input_shape=INPUT_SHAPE, n_class=6):
    # Adding the InputLayer takes the input data (e.g., an image) with the specified input_shape
    input_image = tf.keras.layers.Input(shape=input_shape, dtype=tf.float32, name='image')

    # Load Keras Model as basemodel (convolutional layers) ie the "bottom" in keras terms
    base_model_output = tf.keras.applications.EfficientNetV2B0(
        input_shape=input_shape,   # Specifying the expected input shape
        weights='imagenet',        # (pre-training on ImageNet-1k), or the path to the weights file to be loaded. Defaults to "imagenet"
        include_top=False,         # include the fully-connected layer at the top of the network
        # pooling='avg',           # means that global average pooling will be applied to the output of the last convolutional layer
    )(input_image, training=False) # Freeze the weights during this call, don't want to change top layers

    # Convolutional layer capture hierarchical features
    x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(2, 2), activation='selu')(base_model_output)
    # Add regularization techniques
    x = tf.keras.layers.BatchNormalization()(x)
    # Dropout to helps prevent overfitting by adding noise during training
    x = tf.keras.layers.SpatialDropout2D(0.3)(x)
    # Pooling to down-sample the spatial dimensions and reduce the number of parameters
    x = tf.keras.layers.AveragePooling2D((3, 3))(x)

    # Flatten layer flattens the data into a one-dimensional vector. ( BxHxWxC to Bx(HxWxC) )
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dropout(rate=0.3)(x)
    x = tf.keras.layers.Dense(256, activation='relu')(x)
    x = tf.keras.layers.Dropout(rate=0.3)(x)
    x = tf.keras.layers.Dense(128, activation='gelu')(x)
    x = tf.keras.layers.Dropout(rate=0.3)(x)

    # End with output layer, 3 due to the number of classes
    output = tf.keras.layers.Dense(n_class, activation='softmax')(x)

    # Create the final model
    model = tf.keras.Model(inputs=input_image, outputs=output)

    # Compile the model with specified hyperparameters
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE / 33, weight_decay=LEARNING_RATE),
        loss='categorical_crossentropy',
        metrics=['categorical_accuracy', pFbeta(), 'Precision', 'Recall']
    )
    return model

tf.keras.backend.clear_session()
gc.collect()

with strategy.scope():
    model2 = model_builder()
    model2.summary()

# Training the model
history = model2.fit(
    train_generator, validation_data=val_generator,
    epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=1,
)

plt.plot(history.history['categorical_accuracy'], label='train')
plt.plot(history.history['val_categorical_accuracy'], label='val')
plt.xticks(np.arange(3))
plt.legend();

from google.colab import drive
drive.mount('/content/drive')

"""## Save Model and Weights

- High-level (Keras): [Model.save](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save) and [tf.keras.models.load_model](https://www.tensorflow.org/api_docs/python/tf/keras/saving/load_model) (.keras zip archive format)
- Low-level: [tf.saved_model.save](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) and [tf.saved_model.load](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) (TF SavedModel format)
"""

# Save model, Path where to save the model. (Keras V3 ends in .keras)
# tf.keras.models.load_model('car_tfmodel_v213.h5')
model2.save('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_v213.keras')

# Weights can be saved, Path where to save the model. (Keras V3 ends in .weights.h5)
# model.load_weights('car_tfmodel_v213.weights.h5')
model2.save_weights('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_v213.weights.h5')

# Save model, Path where to save the model. (Keras legacy ends in .h5)
# tf.keras.models.load_model('car_tfmodel_legacy.h5')
model2.save('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_legacy.h5', save_format='h5')

# Weights can be saved, Path where to save the model. (Keras legacy ends in .h5)
# Model.load_weights('breast_cancer_tfmodel_legacy_weights.h5')
model2.save_weights('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_legacy_weights.h5')

"""## Testing model and Prepare for Containerization"""

tf.keras.backend.clear_session()
gc.collect()
with strategy.scope():
    # Load the model (architecture + weights)
    model = tf.keras.models.load_model('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_v213.keras', compile=False)
    # Set the entire model to be non-trainable
    model.trainable = False
    model.compile(metrics=['Precision', 'Recall'])
    # Validation metric on initialized model
    _ = model.evaluate(val_generator, return_dict=True, verbose=1)

"""# Convert Keras to TF-Lite"""

tf.keras.backend.clear_session()
# Load the model (architecture + weights)
model = tf.keras.models.load_model('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_v213.keras', compile=False)
# Set the entire model to be non-trainable
model.trainable = False
model.compile()

# Converting a tf.Keras model to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_v214.tflite', 'wb') as f_out:
    f_out.write(tflite_model)

#create inerpreter
interpreter = tflite.Interpreter(model_path='/content/drive/MyDrive/bcarmulticlassificationcar_tfmodel_v214.tflite')
interpreter.allocate_tensors()
input_index  = interpreter.get_input_details()[0]['index']
output_index = interpreter.get_output_details()[0]['index']

"""## Testing function for Lambda"""

from PIL import Image
import numpy as np
import requests
import io

label_to_cls={1:"crack",
              2:"scratch",
              3:"tire flat",
              4:"dent",
              5:"glass shatter",
              6:"lamp broken"}

def load_and_preprocess_image(url, target_size=(600, 800), debug=False):
    # Fetch the image from the URL
    response = requests.get(url)

    # Open the image, resize, and convert to NumPy array
    img = Image.open(io.BytesIO(response.content)).resize(target_size)
    X = (np.expand_dims(np.array(img), 0) / 255).astype(np.float32)
    if debug:
        print(f'image shape  : {X.shape}, image dtype: {X.dtype}')
        print(f'image min    : {X.min():.2f}, max: {X.max():.2f}')
        plt.imshow(img)
        plt.axis("Off")
    return X


# using random image
url = "https://www.toyotanation.com/attachments/received_252542826473517-jpeg.343498"
X = load_and_preprocess_image(url, debug=True)

interpreter.set_tensor(input_index, X)
interpreter.invoke()
preds = interpreter.get_tensor(output_index)
label_to_cls[np.argmax(preds)+1]

dict(zip(label_to_cls.items(), preds[0]))

"""class with the highest probability:

* crack        : 3.36%
* scratch      : 21.92%
* tire flat    : 8.42%
* dent         : 40.47%
* glass shatter: 19.55%
* lamp broken  : 6.28%
"""

# https://github.com/alexeygrigorev/keras-image-helper
from keras_image_helper import create_preprocessor
# Create preprocessor for xception,resnet50,vgg16,inception_v3, if Needed
preprocessor = create_preprocessor('xception', target_size=(600, 800))

# using random image
url = "https://www.toyotanation.com/attachments/received_252542826473517-jpeg.343498"
X = preprocessor.from_url(url)

interpreter.set_tensor(input_index, X)
interpreter.invoke()
preds = interpreter.get_tensor(output_index)

dict(zip(label_to_cls.items(), preds[0]))

"""## Convert .ipynb to .py

Bash code for converting ipynb to py

```bash
jupyter nbconvert --to script model.ipynb
```

lambda function a function must be added as below to the lambda_function.py file:

```py
def lambda_handler(event, context):
    url = event['url']
    result = predict(url)
    return result
```
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile lambda_function.py
# #!/usr/bin/env python
# # coding: utf-8
# 
# import tflite_runtime.interpreter as tflite
# from keras_image_helper import create_preprocessor
# 
# interpreter = tflite.Interpreter(model_path='car_tfmodel_v214.tflite')
# interpreter.allocate_tensors()
# input_index  = interpreter.get_input_details()[0]['index']
# output_index = interpreter.get_output_details()[0]['index']
# 
# classes = [
#     'crack',
#     'scratch',
#     'tire_flat',
#     'dent',
#     'glass_shatter',
#     'lamp_broken'
# ]
# 
# preprocessor = create_preprocessor('xception', target_size=(600, 800))
# 
# def predict(url):
#     X = preprocessor.from_url(url)
# 
#     interpreter.set_tensor(input_index, X)
#     interpreter.invoke()
#     preds = interpreter.get_tensor(output_index)
# 
#     float_predictions = preds[0].tolist()
# 
#     return dict(zip(classes, float_predictions))
# 
# 
# def lambda_handler(event, context):
#     url    = event['url']
#     result = predict(url)
#     return result

"""## Testing function for Lambda"""

import lambda_function

# Create the event with the URL of the image
event = {'url': 'https://www.toyotanation.com/attachments/received_252542826473517-jpeg.343498'}

# Invoke the Lambda function locally
lambda_function.lambda_handler(event, None)

"""# Preparing Docker Image

- https://repost.aws/knowledge-center/lambda-container-images

Build docker image using the recommended public image for Lambda once Dockerfile has been created below:

```sh
docker build -t car-insurance-model .
```

To test first run image that was built:

```sh
docker run -it --rm -p 8080:8080 car-insurance-model:latest
```
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile Dockerfile
# # public image for Lambda
# FROM public.ecr.aws/lambda/python:3.10
# 
# # Copy the Pipfile and Pipfile.lock into the container
# # COPY ["requirements.txt", "./"]
# # RUN pip install -r requirements.txt
# 
# # recompiled with the lambda image
# RUN pip install -U https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl
# RUN pip install keras-image-helper
# 
# # Copy function code and model into the container
# COPY ["lambda_function.py", "car_tfmodel_v214.tflite", "./"]
# 
# # Set the CMD to your handler
# CMD [ "lambda_function.lambda_handler" ]

# to local
# docker build -t car-insurance-model .
# docker run -it --rm -p 9000:8080 car-insurance-model:latest

"""## test.py created per AWS documentation for testing

Run the file:

```sh
python client_to_docker_test.py
```

This is the output I recieved which clearly shows that the image was predicted as a "dent" which is correct:

```sh
{'crack': 0.006185653153806925,
 'scratch': 0.34056955575942993,
 'tire_flat': 0.021280569955706596,
 'dent': 0.5486962795257568,
 'glass_shatter': 0.0674322172999382,
 'lamp_broken': 0.01583569310605526}
```
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile client_to_docker_test.py
# import requests
# 
# # curl -XPOST "http://localhost:9000/2015-03-31/functions/function/invocations" -d '{}'
# url = 'http://localhost:9000/2015-03-31/functions/function/invocations'
# 
# # Create the event with the URL of the image
# event = {'url': 'https://www.toyotanation.com/attachments/received_252542826473517-jpeg.343498'}
# 
# # Send POST request using requests module
# response = requests.post(url, json=event)
# # Print the response
# print(response.text)

# to local
# python client_to_docker_test.py

# python client_to_docker_test.py
# {"crack": 0.006185653153806925, "scratch": 0.34056955575942993, "tire_flat": 0.021280569955706596, "dent": 0.5486962795257568, "glass_shatter": 0.0674322172999382, "lamp_broken": 0.01583569310605526}

"""# Docker Hub"""

# Tag the Existing Image, username/car-insurance-model:new-tag
# docker tag car-insurance-model:latest username/car-insurance-model:latest

# Push the newly tagged image to Docker Hub:
# username/car-insurance-model:latest

# you can pull the image:
# docker pull username/car-insurance-model:latest

"""## End of Project"""